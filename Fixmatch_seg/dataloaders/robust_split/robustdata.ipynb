{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78b7165c-c066-4c90-84cc-65bca8bb8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torchvision.transforms import Compose, Resize, RandomCrop\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "from numpy import zeros_like\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "class AdaptiveResize():\n",
    "    def __init__(self, size):\n",
    "        self.int_resize = Resize(size, InterpolationMode.NEAREST)\n",
    "        self.float_resize = Resize(size, InterpolationMode.BILINEAR)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if tensor.dtype == torch.long:\n",
    "            return self.int_resize(tensor)\n",
    "        else:\n",
    "            return self.float_resize(tensor)\n",
    "\n",
    "class StatefulRandomCrop():\n",
    "    def __init__(self, size):\n",
    "        self.random_crop = RandomCrop(size)\n",
    "        self.state = None\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if self.state == None:\n",
    "            self.state = torch.random.get_rng_state()\n",
    "        else:\n",
    "            torch.random.set_rng_state(self.state)\n",
    "            self.state = None\n",
    "        return self.random_crop(tensor)\n",
    "\n",
    "def none_transform(x):\n",
    "    return x\n",
    "\n",
    "def prepare_cross_validation_datasets(root_dir, cross_validation_index=0, training_transform=none_transform, validation_transform=none_transform):\n",
    "    \n",
    "    surgery_types = listdir(f\"{root_dir}/Training\")\n",
    "    surgery_indices_per_type = [sorted(map(int, listdir(f\"{root_dir}/Training/{surgery_type}\"))) for surgery_type in surgery_types]\n",
    "\n",
    "    training_surgeries = []\n",
    "    validation_surgeries = []\n",
    "\n",
    "    for surgery_type, indicies in zip(surgery_types, surgery_indices_per_type):\n",
    "        validation_surgeries += [f\"{root_dir}/Training/{surgery_type}/{indicies.pop(cross_validation_index)}\"]\n",
    "        training_surgeries += [f\"{root_dir}/Training/{surgery_type}/{index}\" for index in indicies]\n",
    "\n",
    "    training_dataset = ConcatDataset([RobustDataset(f\"{surgery_path}/*\", training_transform) for surgery_path in training_surgeries])\n",
    "    validation_dataset = ConcatDataset([RobustDataset(f\"{surgery_path}/*\", validation_transform) for surgery_path in validation_surgeries])\n",
    "\n",
    "    return training_dataset, validation_dataset\n",
    "\n",
    "class RobustDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sample_path_glob, common_transform=none_transform, img_transform=none_transform, seg_transform=none_transform):\n",
    "\n",
    "        self.sample_paths = glob(sample_path_glob)\n",
    "\n",
    "        self.img_transform = Compose([common_transform, img_transform])\n",
    "        self.seg_transform = Compose([common_transform, seg_transform])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        sample_path = self.sample_paths[index]\n",
    "\n",
    "        img_path = f\"{sample_path}/raw.png\"\n",
    "        new_string = img_path.split(\"/\",20)\n",
    "        name=new_string[6]+'/'+new_string[7]+'/'+new_string[8]\n",
    "        seg_path = f\"{sample_path}/instrument_instances.png\"\n",
    "\n",
    "        img = np.array(Image.open(img_path))\n",
    "        seg = np.array(Image.open(seg_path)) if exists(seg_path) else zeros_like(img[..., 0])\n",
    "        \n",
    "        size=img.shape\n",
    "\n",
    "        img = torch.from_numpy(img).permute(2, 0, 1) / 255\n",
    "        seg = torch.from_numpy(seg).unsqueeze(0).long()\n",
    "\n",
    "        img = self.img_transform(img)\n",
    "        seg = self.seg_transform(seg)\n",
    "        \n",
    "        seg[seg != 0] = 1\n",
    "        seg = seg.squeeze()\n",
    "        seg = one_hot(seg.squeeze(), num_classes=2).permute(2, 0, 1).float()\n",
    "\n",
    "        return img, seg, np.array(size),name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d398590a-3158-418d-9feb-d9ae82bec5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, CenterCrop\n",
    "\n",
    "training_transform = Compose([AdaptiveResize((270, 480)), StatefulRandomCrop((256, 448))])\n",
    "validation_transform = Compose([AdaptiveResize((270, 480)), CenterCrop((256, 448))])\n",
    "training_dataset, validation_dataset = prepare_cross_validation_datasets(\"/redresearch1/shared/mwei/robustmislite\", 1, training_transform, validation_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4549070b-ccfd-476c-ba62-34e49d1b396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=16, num_workers=16//2, pin_memory=True, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, num_workers=16//2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f2175-53e4-492d-b274-44092f377dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for batch_idx, (img,seg,size,name) in enumerate(training_dataloader):\n",
    "    plt.figure()\n",
    "    plt.imshow(seg[1][1].numpy())\n",
    "    print(np.unique(seg[1][1]))\n",
    "    plt.show()\n",
    "    #print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315014b9-7870-4a52-bb22-0e09514a8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/redresearch1/shared/mwei/robustmislite\"\n",
    "surgery_types=listdir(f\"{root_dir}/Training\")\n",
    "surgery_indices_per_type = [sorted(map(int, listdir(f\"{root_dir}/Training/{surgery_type}\"))) for surgery_type in surgery_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674a8620-ec88-4582-bbcb-605fb58a0015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 8, 9, 10], [1, 2, 3, 6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surgery_indices_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7355973a-fca5-419c-bd57-d9ab14261b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_surgeries = []\n",
    "validation_surgeries = []\n",
    "for surgery_type, indicies in zip(surgery_types, surgery_indices_per_type):\n",
    "        validation_surgeries += [f\"{root_dir}/Training/{surgery_type}/{indicies.pop(1)}\"]\n",
    "        training_surgeries += [f\"{root_dir}/Training/{surgery_type}/{index}\" for index in indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca8a2635-69d3-4a20-9c6c-371a545853ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Training/Rectalresection/10/99000/raw.png'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_temp.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bcc3734-dacc-476e-ac6a-b6e7137a5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=[]\n",
    "label_path=[]\n",
    "with open('val.txt', 'w') as f:\n",
    "    for surgery_path in validation_surgeries:\n",
    "        sample_all=glob(f\"{surgery_path}/*\")\n",
    "        for sample in sample_all:\n",
    "            img_temp=f\"{sample[39:]}/raw.png\"\n",
    "            img_temp=img_temp.replace(' ', '')\n",
    "            f.write(img_temp)\n",
    "          #  img_path.append(img_temp)\n",
    "            label_temp=f\"{sample[39:]}/instrument_instances.png\"\n",
    "            label_temp=label_temp.replace(' ', '')\n",
    "            f.write(' ')\n",
    "            f.write(label_temp)\n",
    "          #  label_path.append(label_temp)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ccc56e9-ad48-43ba-b776-a19d613915a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "lines = open('train.txt').read().splitlines()\n",
    "myline =np.random.choice(lines,1000,replace=False)\n",
    "suline= np.setdiff1d(np.array(lines),np.array(myline))\n",
    "with open('1000_train_supervised.txt', 'w') as f:\n",
    "    for line in myline:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "with open('1000_train_unsupervised.txt', 'w') as f:  \n",
    "    for line in suline:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6894be53-3328-47cd-9ab3-bf494f88a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voc_pallete(num_classes):\n",
    "    n = num_classes\n",
    "    pallete = [0]*(n*3)\n",
    "    for j in range(0,n):\n",
    "            lab = j\n",
    "            pallete[j*3+0] = 0\n",
    "            pallete[j*3+1] = 0\n",
    "            pallete[j*3+2] = 0\n",
    "            i = 0\n",
    "            while (lab > 0):\n",
    "                    pallete[j*3+0] |= (((lab >> 0) & 1) << (7-i))\n",
    "                    pallete[j*3+1] |= (((lab >> 1) & 1) << (7-i))\n",
    "                    pallete[j*3+2] |= (((lab >> 2) & 1) << (7-i))\n",
    "                    i = i + 1\n",
    "                    lab >>= 3\n",
    "    return pallete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f23d34b3-0a3d-4bc3-9b7b-cf2ab25d9243",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2804947235.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9138/2804947235.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2.sub(0.5)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "torch.rand([1,1,1]).sub(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CCT",
   "language": "python",
   "name": "cct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
